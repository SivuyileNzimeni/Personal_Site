{
  "articles": [
    {
      "path": "Ask_A_Manager_Survey.html",
      "title": "Ask A Manager Survey",
      "description": "This post is about the 2021 Ask A Manager Survey analysis. It provides an overview of the data analysis process attempting to fit a linear regression model to explain variances in reported salaries among the respondents\n",
      "author": [
        {
          "name": {},
          "url": "https://www.linkedin.com/in/sivuyile-nzimeni-98006ba0/"
        }
      ],
      "date": "December 25, 2021",
      "contents": "\n\nContents\nINTRODUCTION\nREPRODUCIBILITY\nOUT OF SAMPLE TESTING\n\nLASSO REGRESSION\nIMPORTANT VARIABLE EXTRACTION\nTIDYMODELS: A CLEAN INTERFACE FOR MACHING LEARNING\n\nRESULTS\nTraining Dataset\nTest Dataset\n\nCONCLUSION\nREFERENCES\n\nINTRODUCTION\nIn early 2021, the Ask A Manager blogsite ran their annual salary survey. The survey responses are stored on googlesheets, making the data accessible to all interested. In a previous post, we discussed data pre-processing and feature selection method. This post focuses two aspects,namely, 1) Reproducibility and 2) Out-of-Sample Testing.\nREPRODUCIBILITY\nIn the previous post, we detailed the feature selection method by regularised regreesion, specifically, LASSO regression. In this post, we will attempt to reproduce the feature selection method.\nOUT OF SAMPLE TESTING\nIn this post, we will also follow the traditional machine learning workflow including, splitting the data into a training and testing samples, fitting a model, cross-validation and finally fitting the model on the out-of-sample dataset(testing dataset). This approach can inform us about the model’s parsimony. In other words, can the model perform well on an unknown sample.\n\n\n\n\n\n\nLASSO REGRESSION\nAs mentioned above, the data cleaning process was detailed on an earlier post. It is worth highlighting that the dataset contained several dummy variables indicating various respondent attributes such as job_title, industry, city, age group etc. Our dependent variable is the annual salary. Given the large number of independent variables. In the code below, we fit a LASSO regression model.\n\n\nX <- model.matrix(new_annual_salary ~.,Salary_Train)[,-1]\nY <- log(Salary_Train$new_annual_salary)\n\nlasso <- glmnet(x=X,\n       y= Y,\n       alpha = 1)\n\n\n\nIMPORTANT VARIABLE EXTRACTION\nThe resulting object is a list of class “glmnet”. Essentially, the results contain all the iterations through alpha and to find the minimum lambda. As such, there are several results in the object. We are primarily interested in extracting the remaining variables at minimum lambda along with their coeffiecients. The vip package can automate the plotting of the results. However, we want to extract the variables names in order to fit them on our training test. There is probably a package to assist with this step somewhere in the wild, however, we haven’t found it yet. Luckily in R, we can write custom functions. The code below details the variable_extractor function.\n\n\nvariable_extractor <- function(a_list){\n  min_lambda <- data.frame(best_lambda =a_list[[\"lambda\"]]==min(a_list[[\"lambda\"]]))\n  min_lambda <- cbind.data.frame(data.frame(index = rownames(min_lambda)),\n                                 min_lambda)\n  min_lambda <- min_lambda %>% \n    filter(best_lambda == TRUE)\n  min_lambda <- as.double(unique(min_lambda$index))\n  lasso_variables <- as.matrix(coef(a_list))|>data.frame()\n  lasso_variables <- cbind.data.frame(variables = rownames(lasso_variables),\n                                      lasso_variables)\n  lasso_variables <- tibble(lasso_variables)\n  lasso_variables <- lasso_variables[,c(1,min_lambda+1)]\n  names(lasso_variables) <- c(\"variable\",\"importance\")\n  lasso_variables <- lasso_variables %>% \n    filter(variable != \"(Intercept)\",\n           importance != 0.00000000) %>% \n    arrange(desc(importance))\n\n  return(lasso_variables)\n}\n\nlasso_variable <- variable_extractor(lasso)\n\n\n\nThe function takes a list of attribute “glmnet”. Thereafter, we find the smallest lambda. In addition, we extract all the coefficients from the object and store them as a wide dataframe. The dataframe is subset to only contain the variable name along with the coefficients of the smallest lambda value. We discard the intercept and variable with coefficients that are equal to 0.0000000. The final output is identical to the variables extracted by the vip package. Finally, we subset both the training dataset and the testing dataset to only contain the selected independent variables. Since the outcome variable is expressed in USD terms, we log the outcome variable.\n\n\n\nTIDYMODELS: A CLEAN INTERFACE FOR MACHING LEARNING\nThe R programming language doesn’t not lack methods for running machine learning algorithms, it is after all, a statistical programming language. The tidymodels metapackage aims to provide a standard interface for modelling and machine learning using tidyverse principles. Below, we use the package to complete a number of steps including, crossfold_validation, model specification, fitting on the resamples and finally fitting the model on the training dataset.\n\n\nSalary_Folds <- vfold_cv(Salary_Train)\n\nlm_spec <- linear_reg(engine = \"lm\")\n\nlm_recipe <- recipe(new_annual_salary ~.,Salary_Train) %>% \n  step_nzv(all_predictors())\n\nlm_wf <- workflow(lm_recipe,lm_spec)\n\ndoParallel::registerDoParallel(cores = 10)\nctrl_preds <- control_resamples(save_pred = TRUE)\ncv_results <- fit_resamples(lm_wf,Salary_Folds,control = ctrl_preds)\n\nlm_wf <- fit(lm_wf,Salary_Train)\n\ncollect_metrics(cv_results,summarize = FALSE) %>% \n  filter(.metric == \"rsq\") %>% \n  summarise(avg_estimate = mean(.estimate),\n            .groups = \"drop\")\n\n\n# A tibble: 1 × 1\n  avg_estimate\n         <dbl>\n1        0.269\n\nAcross all 10 validation folds, the linear regression model’s adjusted R-square averaged 0.272. It is possible to tune the parameters and use battery of other machine learning models to improve performance. In the previous post, we utilised random forest to try an improve the model. Despite, requiring additional computational power, the increases in peformance were marginal to neglible.\nAt this point, we have completed our first objective. We are able to reproduce the LASSO regresison results in the previous post. The next objective is to determine the parsimony of the model. Here, we fit model on the test data.\n\n\n\n\n\n\n\n\n\n\n\n\nRESULTS\nTraining Dataset\n\n\n@import url(\"https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap\");\n@import url(\"https://fonts.googleapis.com/css2?family=Libre+Franklin:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap\");\n@import url(\"https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap\");\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#qesgnpbkeh .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: none;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#qesgnpbkeh .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#qesgnpbkeh .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#qesgnpbkeh .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#qesgnpbkeh .gt_bottom_border {\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#qesgnpbkeh .gt_col_headings {\n  border-top-style: none;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: none;\n  border-bottom-width: 1px;\n  border-bottom-color: #334422;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#qesgnpbkeh .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#qesgnpbkeh .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#qesgnpbkeh .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#qesgnpbkeh .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#qesgnpbkeh .gt_column_spanner {\n  border-bottom-style: none;\n  border-bottom-width: 1px;\n  border-bottom-color: #334422;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#qesgnpbkeh .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#qesgnpbkeh .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#qesgnpbkeh .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#qesgnpbkeh .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#qesgnpbkeh .gt_row {\n  padding-top: 7px;\n  padding-bottom: 7px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#qesgnpbkeh .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#qesgnpbkeh .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#qesgnpbkeh .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#qesgnpbkeh .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#qesgnpbkeh .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#qesgnpbkeh .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#qesgnpbkeh .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#qesgnpbkeh .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#qesgnpbkeh .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#qesgnpbkeh .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#qesgnpbkeh .gt_table_body {\n  border-top-style: none;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #FFFFFF;\n}\n\n#qesgnpbkeh .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#qesgnpbkeh .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-left: 4px;\n  padding-right: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#qesgnpbkeh .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#qesgnpbkeh .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#qesgnpbkeh .gt_left {\n  text-align: left;\n}\n\n#qesgnpbkeh .gt_center {\n  text-align: center;\n}\n\n#qesgnpbkeh .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#qesgnpbkeh .gt_font_normal {\n  font-weight: normal;\n}\n\n#qesgnpbkeh .gt_font_bold {\n  font-weight: bold;\n}\n\n#qesgnpbkeh .gt_font_italic {\n  font-style: italic;\n}\n\n#qesgnpbkeh .gt_super {\n  font-size: 65%;\n}\n\n#qesgnpbkeh .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 75%;\n  vertical-align: 0.4em;\n}\n\n#qesgnpbkeh .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#qesgnpbkeh .gt_slash_mark {\n  font-size: 0.7em;\n  line-height: 0.7em;\n  vertical-align: 0.15em;\n}\n\n#qesgnpbkeh .gt_fraction_numerator {\n  font-size: 0.6em;\n  line-height: 0.6em;\n  vertical-align: 0.45em;\n}\n\n#qesgnpbkeh .gt_fraction_denominator {\n  font-size: 0.6em;\n  line-height: 0.6em;\n  vertical-align: -0.05em;\n}\nLinear Regression Model: Training Dataset\n    Depedent Variable: log(Annual Salary(USD))\n    Characteristic\n      Beta (SE)1,2\n    (Intercept)\n11***\n(0.022)years_of_experience_in_field_X21...30.years\n0.40***\n(0.023)job_title_director\n0.31***\n(0.013)years_of_experience_in_field_X11...20.years\n0.30***\n(0.019)highest_level_of_education_completed_PhD\n0.27***\n(0.017)city_york\n0.26***\n(0.016)years_of_experience_in_field_X8...10.years\n0.24***\n(0.020)job_title_engineer\n0.23***\n(0.016)industry_government\n0.19*\n(0.082)industry_tech\n0.26\n(0.197)job_title_senior\n0.16***\n(0.012)years_of_experience_in_field_X5.7.years\n0.14***\n(0.019)how_old_are_you_X45.54\n0.12***\n(0.019)industry_computing\n0.03\n(0.197)how_old_are_you_X35.44\n0.12***\n(0.018)job_title_manager\n0.11***\n(0.010)industry_health\n0.14\n(0.084)industry_engineering\n0.08***\n(0.017)industry_administration\n0.08\n(0.090)industry_finance\n0.33\n(0.216)how_old_are_you_X25.34\n0.07***\n(0.017)job_title_analyst\n0.04**\n(0.015)highest_level_of_education_completed_Master.s.degree\n0.04***\n(0.008)years_of_experience_in_field_X2...4.years\n0.03\n(0.019)industry_banking\n-0.27\n(0.216)industry_care\n-0.06\n(0.084)gender_Woman\n-0.03**\n(0.009)race_White\n-0.11***\n(0.010)overall_years_of_professional_experience_X8...10.years\n-0.12***\n(0.023)overall_years_of_professional_experience_X11...20.years\n-0.13***\n(0.022)overall_years_of_professional_experience_X21...30.years\n-0.13***\n(0.023)overall_years_of_professional_experience_X5.7.years\n-0.16***\n(0.024)overall_years_of_professional_experience_X2...4.years\n-0.18***\n(0.023)industry_nonprofits\n-0.18***\n(0.014)industry_education\n-0.18***\n(0.013)job_title_assistant\n-0.22***\n(0.015)highest_level_of_education_completed_Some.college\n-0.22***\n(0.014)industry_public\n-0.24***\n(0.053)Adjusted R-square: 0.2732\n    1 *p<0.05; **p<0.01; ***p<0.001\n    2 SE = Standard Error\n    \n\nTest Dataset\n\n\n@import url(\"https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap\");\n@import url(\"https://fonts.googleapis.com/css2?family=Libre+Franklin:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap\");\n@import url(\"https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap\");\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#ljiaojwlkk .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: none;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#ljiaojwlkk .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#ljiaojwlkk .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#ljiaojwlkk .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#ljiaojwlkk .gt_bottom_border {\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#ljiaojwlkk .gt_col_headings {\n  border-top-style: none;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: none;\n  border-bottom-width: 1px;\n  border-bottom-color: #334422;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#ljiaojwlkk .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#ljiaojwlkk .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#ljiaojwlkk .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#ljiaojwlkk .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#ljiaojwlkk .gt_column_spanner {\n  border-bottom-style: none;\n  border-bottom-width: 1px;\n  border-bottom-color: #334422;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#ljiaojwlkk .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#ljiaojwlkk .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#ljiaojwlkk .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#ljiaojwlkk .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#ljiaojwlkk .gt_row {\n  padding-top: 7px;\n  padding-bottom: 7px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#ljiaojwlkk .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#ljiaojwlkk .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#ljiaojwlkk .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#ljiaojwlkk .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#ljiaojwlkk .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#ljiaojwlkk .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#ljiaojwlkk .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#ljiaojwlkk .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#ljiaojwlkk .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#ljiaojwlkk .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#ljiaojwlkk .gt_table_body {\n  border-top-style: none;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #FFFFFF;\n}\n\n#ljiaojwlkk .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#ljiaojwlkk .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-left: 4px;\n  padding-right: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#ljiaojwlkk .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#ljiaojwlkk .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#ljiaojwlkk .gt_left {\n  text-align: left;\n}\n\n#ljiaojwlkk .gt_center {\n  text-align: center;\n}\n\n#ljiaojwlkk .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#ljiaojwlkk .gt_font_normal {\n  font-weight: normal;\n}\n\n#ljiaojwlkk .gt_font_bold {\n  font-weight: bold;\n}\n\n#ljiaojwlkk .gt_font_italic {\n  font-style: italic;\n}\n\n#ljiaojwlkk .gt_super {\n  font-size: 65%;\n}\n\n#ljiaojwlkk .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 75%;\n  vertical-align: 0.4em;\n}\n\n#ljiaojwlkk .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#ljiaojwlkk .gt_slash_mark {\n  font-size: 0.7em;\n  line-height: 0.7em;\n  vertical-align: 0.15em;\n}\n\n#ljiaojwlkk .gt_fraction_numerator {\n  font-size: 0.6em;\n  line-height: 0.6em;\n  vertical-align: 0.45em;\n}\n\n#ljiaojwlkk .gt_fraction_denominator {\n  font-size: 0.6em;\n  line-height: 0.6em;\n  vertical-align: -0.05em;\n}\nLinear Regression Model: Test Dataset\n    Depedent Variable: log(Annual Salary(USD))\n    Characteristic\n      Beta (SE)1,2\n    (Intercept)\n11***\n(0.039)years_of_experience_in_field_X21...30.years\n0.39***\n(0.040)job_title_director\n0.35***\n(0.023)years_of_experience_in_field_X11...20.years\n0.26***\n(0.033)highest_level_of_education_completed_PhD\n0.24***\n(0.029)city_york\n0.22***\n(0.028)years_of_experience_in_field_X8...10.years\n0.21***\n(0.033)job_title_engineer\n0.27***\n(0.027)industry_government\n0.48***\n(0.133)industry_tech\n-0.04\n(0.281)job_title_senior\n0.14***\n(0.020)years_of_experience_in_field_X5.7.years\n0.13***\n(0.032)how_old_are_you_X45.54\n0.07\n(0.035)industry_computing\n0.30\n(0.281)how_old_are_you_X35.44\n0.06*\n(0.032)job_title_manager\n0.14***\n(0.017)industry_health\n0.19\n(0.128)industry_engineering\n0.10**\n(0.029)industry_administration\n-0.20\n(0.142)industry_finance\n0.11\n(0.279)how_old_are_you_X25.34\n0.04\n(0.030)job_title_analyst\n0.05\n(0.027)highest_level_of_education_completed_Master.s.degree\n0.04**\n(0.014)years_of_experience_in_field_X2...4.years\n0.01\n(0.031)industry_banking\n-0.08\n(0.280)industry_care\n-0.13\n(0.128)gender_Woman\n-0.04**\n(0.016)race_White\n-0.12***\n(0.017)overall_years_of_professional_experience_X8...10.years\n-0.12**\n(0.043)overall_years_of_professional_experience_X11...20.years\n-0.07\n(0.041)overall_years_of_professional_experience_X21...30.years\n-0.08\n(0.041)overall_years_of_professional_experience_X5.7.years\n-0.13**\n(0.043)overall_years_of_professional_experience_X2...4.years\n-0.22***\n(0.041)industry_nonprofits\n-0.20***\n(0.024)industry_education\n-0.20***\n(0.022)job_title_assistant\n-0.22***\n(0.026)highest_level_of_education_completed_Some.college\n-0.18***\n(0.025)industry_public\n-0.24**\n(0.089)Adjusted R-square: 0.2624\n    1 *p<0.05; **p<0.01; ***p<0.001\n    2 SE = Standard Error\n    \n\nCONCLUSION\nThe model performs well on an out-of-sample dataset with year of experience (21 - 30 years), job title (Director) and highest level of education (PhD) being among the most important predictors of annual salary. Provided the questions utilised in 2022 are similar or identical to the 2021 survey, the model above can be evaluated on the 2022 survey results.\nREFERENCES\nSilge,J. 2021.Fit and predict with tidymodels for #TidyTuesday bird baths in Australia. Available From: https://juliasilge.com/blog/bird-baths/ (Accessed 24 December 2021).\nWickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686\nSam Firke (2021). janitor: Simple Tools for Examining and Cleaning Dirty Data. R package version 2.1.0. https://CRAN.R-project.org/package=janitor\nKuhn et al., (2020). Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org\nJerome Friedman, Trevor Hastie, Robert Tibshirani (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22. https://www.jstatsoft.org/v33/i01/.\nRichard Iannone, Joe Cheng and Barret Schloerke (2021). gt: Easily Create Presentation-Ready Display Tables. R package version 0.3.1. https://CRAN.R-project.org/package=gt\nThomas Mock (2021). gtExtras: A Collection of Helper Functions for the gt Package. R package version 0.2.2.11. https://github.com/jthomasmock/gtExtras\nSjoberg DD, Whiting K, Curry M, Lavery JA, Larmarange J. Reproducible summary tables with the gtsummary package. The R Journal 2021;13:570–80. https://doi.org/10.32614/RJ-2021-053.\n\n\n\n",
      "last_modified": "2022-03-01T03:32:32+02:00"
    },
    {
      "path": "Cars_SA.html",
      "title": "South African Car Prices",
      "description": "This post details the data scraping process for obtaining data from a South African online vehicle marketplate. In addition, the post shares the results of a linear regression model to estimate determinants of vehicle prices.\n",
      "author": [
        {
          "name": {},
          "url": "https://www.linkedin.com/in/sivuyile-nzimeni-98006ba0/"
        }
      ],
      "date": "February 28, 2022",
      "contents": "\n\nContents\nINTRODUCTION\nWeb SCRAPING\nIMPORTING THE DATA\nDATA PREPROCESSING\nLASSO IT ONCE, LASSO IT UNTIL YOU CAN ALSO NO MORE\nCROSS VALIDATION\n\nTESTING PERFORMANCE OF A OUT-OF-SAMPLE SET\nCONCLUSION\nREFERENCE\n\nINTRODUCTION\nSouth Africa has a plethora of online vehicle marketplaces. Often, their pool of vehicles for sale are usually > 50 000 on a daily basis. The vehicle listings offer a vast amount of car related data. Naturally, web-scraping the data provides an opportunity to fit a Machine-Learning model and endless exploration for petrol-heads (such as myself). Nearly all the online vehicle marketplaces have restrictive Terms and Conditions deterring the use of their data for commercial purposes and bombarding of their servers through web scrapping among other restrictions. Unfortunately, this means web scraping script cannot be shared in this post as it may reveal where the data were obtained, methodology of scraping the data etc. In addition, the website of the online vehicle marketplace will not be revealed.\nWeb SCRAPING\nTo respect the Terms of Conditions, the scraping was throttled through the use of base R’s rpois function along with the Sys.sleep() function. Combining the two functions results in non-normal distribution of system sleep. System sleep essentially pauses R for the specified period. This step in the scrapping process is important to avoid sending to many requests to a website, perhaps, hoarding server capacity for other users. In this instance,scrapping all the webpages and pulling all listed vehicles on the day(2021-09-02), took 10 hours.\n\n\npause <- function(){\n  Sys.sleep(rpois(1,60))\n  print(paste0(\"Proceeding to the next page...\"))\n  }\n\n\n\nIMPORTING THE DATA\nBelow, we import the data and use dplyr::glimpse function to see the number of columns and values. The dataset contains a few important variables including the car_name and vehicle manufacturer. It is worth noting that the car_name variable appears to a free text field where the person listing the vehicle can modify the car name to include marketing terms such as: “excellent condition”,“reduce price” etc.\n\n\nCar_Data <- read_csv(file =\"./2021-09-03_MANY_VEHICLES_Clean_Db.csv\") %>% \n  clean_names()\n\nglimpse(Car_Data)\n\n\nRows: 62,196\nColumns: 10\n$ car_name             <chr> \"9-3 Sport 2.0 Linear Lpt\", \"S40 2.0T\",…\n$ vehicle_manufacturer <chr> \"Saab\", \"Volvo\", \"Volvo\", \"Hyundai\", \"V…\n$ year                 <dbl> 2007, 1999, 2001, 2000, 2000, 1998, 199…\n$ mileage              <dbl> 200000, 285000, 271000, 125000, 190000,…\n$ price                <dbl> 13700, 18900, 18900, 20000, 20900, 2190…\n$ fuel_type            <chr> \"Petrol\", \"Petrol\", \"Petrol\", \"Petrol\",…\n$ transmission         <chr> \"Manual\", \"Manual\", \"Manual\", \"Manual\",…\n$ dealership           <chr> \"Mahala Motors\", \"WeBuyCars Midstream\",…\n$ city_town            <chr> \"Klerksdorp\", \"Centurion\", \"Cape Town\",…\n$ province             <chr> \"North West Province\", \"Gauteng\", \"West…\n\nSimilarly, the free text field, also means that vehicle naming conventions deviate from the vehicle manufacturer specifications. Other text variables also contain these anomalous changes in text. The code below demonstrates an example of idiosyncrasies.\n\n\nCar_Data %>% \n  filter(str_detect(car_name,\"condition\"))\n\n\n# A tibble: 4 × 10\n  car_name             vehicle_manufac…  year mileage  price fuel_type\n  <chr>                <chr>            <dbl>   <dbl>  <dbl> <chr>    \n1 Focus Excellent con… Ford              2008  305000  69950 Petrol   \n2 Utility 1.4 aircond… Chevrolet         2015  198000 114900 Petrol   \n3 Clio Excellent cond… Renault           2019   55000 160000 Petrol   \n4 Ranger Excellent co… Ford              2017  107000 299900 Diesel   \n# … with 4 more variables: transmission <chr>, dealership <chr>,\n#   city_town <chr>, province <chr>\n\nDATA PREPROCESSING\nWe would like to fit a model to better understand the determinants of vehicle prices. Here, pre-processing is important is especially important. Tidymodels and the textrecipes offer a range of functions to handle the whole modelling workflow. Below, are a number of pre-processing steps, firstly we use the unnest_tokens function from the tidytext package to process the the car_name variable. Thereafter, we handle the numerical values by applying a logarithm to outcome variable and standardising mileage. Finally, we use step_tokenize, step_tokenfilter and step_tf to tokenise, filter the those tokens and ultimately convert the tokens to a term frequency variables.\n\n\nUseful_Car_Names <- Car_Data %>% \n  unnest_tokens(car_name,\n                output=\"car_name\") %>% \n  group_by(vehicle_manufacturer,car_name) %>% \n  summarise(n(),.groups = \"drop\") %>% \n  arrange(desc(`n()`)) %>% \n  anti_join(stop_words %>% \n              rename(car_name=word)) %>% \n  filter(!str_detect(car_name,\"\\\\d{1,}\"))\n\nCar_Data <- Car_Data %>% \n  unnest_tokens(car_name,output = \"car_name\") %>% \n  semi_join(Useful_Car_Names %>% \n              select(car_name,vehicle_manufacturer)) %>%\n  group_by(across(-car_name)) %>% \n  summarise(car_name = as.character(list(c(car_name))),\n            .groups = \"drop\") %>% \n  mutate(car_name = str_replace(car_name,\n                                'c[[:punct:]]{2,}',\"\"),\n         car_name = str_replace_all(car_name,\n                                    '\\\\\"',\"\"),\n         car_name = str_replace_all(car_name,\n                                    '[[:punct:]]{1,}$',\"\"),\n         age = 2021-year)\nCar_Data <- Car_Data %>% \n  mutate(across(c(car_name,dealership,city_town),\n                .fns = as_factor))\n\nNew_Car_Data <- recipe(price ~ .,data = Car_Data) %>% \n  step_log(all_outcomes()) %>%\n  step_normalize(mileage) %>% \n  step_tokenize(c(dealership,city_town,car_name)) %>%\n  step_tokenfilter(c(dealership,city_town,car_name)) %>%\n  step_tf(c(dealership,city_town,car_name)) %>% \n  step_dummy(c(vehicle_manufacturer,province,\n               fuel_type,transmission)) %>% \n  step_nzv(all_predictors()) %>% \n  prep() %>% \n  bake(Car_Data)\n\n\n\nLASSO IT ONCE, LASSO IT UNTIL YOU CAN ALSO NO MORE\nThe resulting dataset contains 59417 rows across 27 variables. Given the dimension above, it prudent to do some additional feature select. LASSO regression helps with variable selection. In turn, we use the LASSO regression results to filter for the appropriate variables. The final variable set yields 24 predictor variables. The code below contains all details the LASSO implementation and subsequent filtering.\n\n\nNew_Car_Data <- New_Car_Data %>%\n  select(-year)\n\nCar_split <- initial_split(New_Car_Data)\nCar_Training <- training(Car_split)\nCar_Test <- testing(Car_split)\n\nX <- model.matrix(price~.,Car_Training)[,-1]\nY <- Car_Training$price\nlasso_model <- glmnet(x = X,y=Y,\n       alpha=1)\n\nvariable_extractor <- function(a_list){\n  min_lambda <- data.frame(best_lambda =a_list[[\"lambda\"]]==min(a_list[[\"lambda\"]]))\n  min_lambda <- cbind.data.frame(data.frame(index = rownames(min_lambda)),\n                                 min_lambda)\n  min_lambda <- min_lambda %>% \n    filter(best_lambda == TRUE)\n  min_lambda <- as.double(unique(min_lambda$index))\n  lasso_variables <-\n    as.matrix(coef(a_list))|>data.frame()\n  \n  lasso_variables <- cbind.data.frame(variables = rownames(lasso_variables),\n                                      lasso_variables)\n  \n  lasso_variables <- tibble(lasso_variables)\n  lasso_variables <- lasso_variables[,c(1,min_lambda+1)]\n  names(lasso_variables) <- c(\"variable\",\"importance\")\n  lasso_variables <- lasso_variables %>% \n    filter(variable != \"(Intercept)\",\n           importance != 0.00000000) %>% \n    arrange(desc(importance))\n\n  return(lasso_variables)\n}\n\nVariables <- variable_extractor(lasso_model)\nCar_Training <- Car_Training[,c(\"price\",Variables$variable)]\nCar_Test <- Car_Test[,c(\"price\",Variables$variable)]\n\n\n\nCROSS VALIDATION\nBefore fitting to the test dataset, it worth investigating whether our model performs well against “shuffled” dataset of our training data. Fortunately, tidymodels and parsnip contain several functions to assist with the exercise.\nHere, we use the vfold_cv function to split our training data into random splits of equal size. Next, we use workflow to fit a linear model on the random splits. Subsequently, we plot the r-squares across all the folds.\nUltimately, cross validation helps us understand the performance of our model set of datasets by iterating through the training and test sample of each fold. The code below illustrates an implementation of cross validation.\n\n\nCar_Training_cv <- vfold_cv(Car_Training)\n\nlinear_model <- linear_reg() %>% \n  set_engine(\"lm\")\n\ncv_outcomes <- workflow() %>% \n  add_model(linear_model) %>% \n  add_formula(price ~.) %>% \n  fit_resamples(Car_Training_cv) %>% \n  select(id,.metrics) %>% \n  unnest(.metrics) %>% \n  filter(.metric == \"rsq\") %>% \n  select(id,.metric,.estimator,.estimate)\n\ncv_outcomes %>%  \n  ggplot(aes(id,.estimate,group=1,fill=id))+\n  geom_col(show.legend = FALSE,alpha=0.8)+\n  geom_text(aes(label=round(.estimate,2)))+\n  coord_flip()+\n  labs(title = \"Cross Validation Results\",\n       subtitle = \"Car Prices Linear Model: R-Square across 10 folds\",\n       x=NULL,\n       y = \"R-Square\")+\n  theme(plot.title = element_text(family = \"Arial Narrow\",\n                                  hjust = 0.5),\n        plot.subtitle = element_text(family = \"Arial Narrow\",\n                                     hjust = 0.5,face = \"italic\"))\n\n\n\n\n(#fig:Cross_Validation)A bar plot containing cross-validated r-squares across 10 folds with an average of 0.63\n\n\n\nTESTING PERFORMANCE OF A OUT-OF-SAMPLE SET\nOur model had an mean r-square of 0.62. Indicating that 62% of the variation in vehicle prices is explained by the variables in our dataset. To test whether the r-square remains stable on an unseen dataset, we fit the model to our test sample. The table below illustrates the results of linear regression below.\n\n\n \n\n\nprice\n\n\nPredictors\n\n\nEstimates\n\n\nstd. Beta\n\n\nCI\n\n\nstandardized CI\n\n\np\n\n\n(Intercept)\n\n\n13.39\n\n\n0.00\n\n\n13.36 – 13.42\n\n\n-0.01 – 0.01\n\n\n<0.001\n\n\nvehicle manufacturerMercedes Benz\n\n\n0.34\n\n\n0.13\n\n\n0.31 – 0.37\n\n\n0.12 – 0.14\n\n\n<0.001\n\n\nvehicle manufacturer BMW\n\n\n0.27\n\n\n0.09\n\n\n0.24 – 0.30\n\n\n0.08 – 0.10\n\n\n<0.001\n\n\ntf car name tsi\n\n\n0.20\n\n\n0.07\n\n\n0.17 – 0.23\n\n\n0.06 – 0.09\n\n\n<0.001\n\n\nvehicle manufacturerToyota\n\n\n0.20\n\n\n0.11\n\n\n0.18 – 0.23\n\n\n0.10 – 0.12\n\n\n<0.001\n\n\ntf car name double\n\n\n0.13\n\n\n0.05\n\n\n0.09 – 0.17\n\n\n0.04 – 0.07\n\n\n<0.001\n\n\nvehicle manufacturerVolkswagen\n\n\n0.10\n\n\n0.05\n\n\n0.06 – 0.13\n\n\n0.03 – 0.07\n\n\n<0.001\n\n\ntf city town pretoria\n\n\n0.01\n\n\n0.01\n\n\n-0.01 – 0.03\n\n\n-0.00 – 0.02\n\n\n0.159\n\n\nvehicle manufacturer Ford\n\n\n0.05\n\n\n0.02\n\n\n0.03 – 0.07\n\n\n0.01 – 0.03\n\n\n<0.001\n\n\ntf car name cab\n\n\n0.02\n\n\n0.01\n\n\n-0.02 – 0.05\n\n\n-0.01 – 0.02\n\n\n0.329\n\n\nmileage\n\n\n-1.39\n\n\n-0.18\n\n\n-1.51 – -1.27\n\n\n-0.19 – -0.16\n\n\n<0.001\n\n\nvehicle manufacturerNissan\n\n\n0.01\n\n\n0.00\n\n\n-0.02 – 0.04\n\n\n-0.01 – 0.01\n\n\n0.637\n\n\nprovince Western Cape\n\n\n-0.01\n\n\n-0.01\n\n\n-0.03 – 0.01\n\n\n-0.02 – 0.00\n\n\n0.273\n\n\ntf dealership cars\n\n\n-0.00\n\n\n-0.00\n\n\n-0.02 – 0.02\n\n\n-0.01 – 0.01\n\n\n0.781\n\n\nprovince Kwazulu Natal\n\n\n-0.01\n\n\n-0.01\n\n\n-0.03 – 0.01\n\n\n-0.01 – 0.00\n\n\n0.289\n\n\ntf dealership motors\n\n\n-0.00\n\n\n-0.00\n\n\n-0.02 – 0.01\n\n\n-0.01 – 0.01\n\n\n0.594\n\n\nvehicle manufacturerHyundai\n\n\n-0.05\n\n\n-0.02\n\n\n-0.07 – -0.02\n\n\n-0.03 – -0.01\n\n\n0.001\n\n\ntf dealership toyota\n\n\n-0.09\n\n\n-0.03\n\n\n-0.11 – -0.06\n\n\n-0.04 – -0.02\n\n\n<0.001\n\n\ntf car name dr\n\n\n-0.09\n\n\n-0.04\n\n\n-0.11 – -0.07\n\n\n-0.05 – -0.03\n\n\n<0.001\n\n\nage\n\n\n-0.06\n\n\n-0.41\n\n\n-0.06 – -0.06\n\n\n-0.42 – -0.39\n\n\n<0.001\n\n\ntf city town johannesburg\n\n\n-0.14\n\n\n-0.06\n\n\n-0.16 – -0.12\n\n\n-0.07 – -0.05\n\n\n<0.001\n\n\ntf car name polo\n\n\n-0.11\n\n\n-0.05\n\n\n-0.15 – -0.08\n\n\n-0.07 – -0.04\n\n\n<0.001\n\n\ntf car name auto\n\n\n-0.29\n\n\n-0.22\n\n\n-0.31 – -0.27\n\n\n-0.23 – -0.20\n\n\n<0.001\n\n\nfuel type Petrol\n\n\n-0.40\n\n\n-0.27\n\n\n-0.42 – -0.38\n\n\n-0.28 – -0.26\n\n\n<0.001\n\n\ntransmission Manual\n\n\n-0.70\n\n\n-0.51\n\n\n-0.73 – -0.68\n\n\n-0.53 – -0.50\n\n\n<0.001\n\n\nObservations\n\n\n14855\n\n\nR2 / R2 adjusted\n\n\n0.678 / 0.677\n\n\nAccording to the results above, statistically significant variables include the vehicle manufacturer, vehicle name,vehicle age,transmission_type and fuel_type. Top among these are Mercedes Benz, BMW,Toyota and Volkswagen. Vehicles with the terms such as: “tsi”,“double”,“cab”,“dr” and “auto”. Interesting, “tsi” is a fuel type designation among Volkswagen group vehicles. Similarly, “double” and “cab” usually denote the bakkie (pickup truck) segment in South Africa. Other vehicle segment key word is “dr” which usually indicate a coupe or two-door saloon vehicle.\nCONCLUSION\nIn example, we have found the determinants of vehicle prices from the scraped dataset. The results are in line with vehicle sales in South Africa. Mercedes Benz, BMW, Toyota and Volkswagen, Ford and Hyundai vehicles constantly feature among top selling vehicles in South Africa. As expected, vehicle age and vehicle mileage are statistically significant factors that influence prices. To enhance the analysis, it may be worthwhile to scrape more variables such as vehicle description, listing date, options list, vehicle conditions and colour.\nREFERENCE\nWickham et al.,2019. Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686\nSam Firke, 2021. janitor: Simple Tools for Examining and Cleaning Dirty Data. R package version 2.1.0. https://CRAN.R-project.org/package=janitor\nHadley Wickham and Jennifer Bryan, 2019. readxl: Read Excel Files. R package version 1.3.1. https://CRAN.R-project.org/package=readxl\nJeroen Ooms, 2021. writexl: Export Data Frames to Excel ‘xlsx’ Format. R package version 1.4.0. https://CRAN.R-project.org/package=writexl\nJeffrey B. Arnold. 2021. ggthemes: Extra Themes, Scales and Geoms for ‘ggplot2’. R package version 4.2.4. https://CRAN.R-project.org/package=ggthemes\nKuhn et al., 2020. Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org\nEmil Hvitfeldt. 2021. textrecipes: Extra ‘Recipes’ for Text Processing. R package version 0.4.1. https://CRAN.R-project.org/package=textrecipes\nSilge J, Robinson, D. 2016. “tidytext: Text Mining and Analysis Using Tidy Data Principles in R.” JOSS, 1(3). doi: 10.21105/joss.00037. https://doi.org/10.21105/joss.00037\nJerome Friedman, Trevor Hastie, Robert Tibshirani. 2010. Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22. https://www.jstatsoft.org/v33/i01/\nJulia Silge, Fanny Chow, Max Kuhn and Hadley Wickham. 2021. rsample: General Resampling Infrastructure. R package version 0.1.1. https://CRAN.R-project.org/package=rsample\nMax Kuhn and Davis Vaughan (2021). parsnip: A Common API to Modeling and Analysis Functions. https://parsnip.tidymodels.org, https://github.com/tidymodels/parsnip\nLüdecke D. 2021. sjPlot: Data Visualization for Statistics in Social Science. R package version 2.8.10. https://CRAN.R-project.org/package=sjPlot\nLüdecke D. 2021. sjlabelled: Labelled Data Utility Functions (Version 1.1.8). doi: 10.5281/zenodo.1249215. https://doi.org/10.5281/zenodo.1249215.\n\n\n\n",
      "last_modified": "2022-03-01T03:32:52+02:00"
    },
    {
      "path": "index.html",
      "title": "Sivuyile Nzimeni",
      "author": [],
      "contents": "\n\n          \n          \n          Sivuyile Nzimeni\n          \n          \n          Home\n          \n          \n          Posts\n           \n          ▾\n          \n          \n          Ask A Manager Survey\n          Scraping DBE School Data\n          State of Capture Part 1: Notes\n          South African Car Prices\n          \n          \n          ☰\n          \n          \n      \n        \n          \n            \n              \n            \n              Sivuyile Nzimeni\n            \n            \n              \n                \n                    \n                      \n                        LinkedIn\n                      \n                    \n                  \n                                    \n                    \n                      \n                        Twitter\n                      \n                    \n                  \n                                    \n                    \n                      \n                        Email\n                      \n                    \n                  \n                                  \n            \n          \n        \n        \n        \n          \n            I am a Data Analyst and Teaching and Learning Coordinator for the Faculty of Economic and Management Sciences at the University of the Free State. Previously worked as a part-time lecturer (Personal Selling), Research Assistant and Intern at the same institution. I have a demonstrated history of working in the higher education industry with expertise in managing a large cohort of employees, Coordinating software development, Data Analysis Financial management, Monitoring and Evaluation, Report Writing and Presentation. In addition, I am skilled in the R Programming Language, Python, KNIME and Julia Programming language among others.\n            \n            Qualifications\n            2016: Bachelor of Commerce (Honours) with specialisation in Entrepreneurial Management (University of the Free State)\n            2015: Bachelor of Arts majoring in Philosophy and Management (University of the Free State)\n            2011: National Senior Certificate (Hoerskool De Vos Malan)\n            \n            \n            Certifications\n            Completed Certifications\n            Year\n            Month\n            Course Description\n            Provider\n            2021\n            Sept\n            Learning Julia (Programming Language)\n            LinkedIn Learning\n            2021\n            Aug\n            Artificial Intelligence Tools and Concepts\n            LinkedIn\n            Learning\n            2021\n            Jun\n            Python for Everybody Specialisation\n            Coursera\n            2021\n            May\n            PHP Essential Training\n            LinkedIn Learning\n            2021\n            Mar\n            Python Data Structures\n            Coursera\n            2021\n            Feb\n            Programming for Everybody(Getting Started with Python)\n            Coursera\n            2020\n            Oct\n            Advertising on Facebook: Advanced\n            LinkedIn Learning\n            2020\n            Sept\n            Developing Data Products\n            Coursera\n            2020\n            Sept\n            PostreSQL Essential Training\n            LinkedIn Learning\n            2020\n            Sept\n            Practical Machine Learning\n            Coursera\n            2020\n            Aug\n            Advanced R Programming\n            Coursera\n            2020\n            Jun\n            Regression Models\n            Coursera\n            2020\n            Apr\n            Data Visualisation in R with ggplot2\n            LinkedIn Learning\n            2020\n            Jan\n            R Statistics Essential Training\n            LinkedIn\n            Learning\n            2019\n            Dec\n            The R Programming Environment\n            Coursera\n            2017\n            Dec\n            Supplemental Instruction Supervisor Training Workshop\n            Nelson Mandela University( The Supplemental Instruction National Office)\n            2017\n            Mar\n            Advanced Project Management (NQF Level 5)\n            Extribyte (Pty) Ltd\n            Public ServiceSector Education & Training Authority\n            \n            \n            Skills\n            Process Automation\n            Data Analysis\n            Management\n            Customer Service\n            Big Data\n            Project Management\n            Database Administration\n            Data Visualisation\n            Web Application\n            Artificial Intelligence\n            \n            \n            Technologies\n            Microsoft365 (Excel,Word,Powerpoint,OneDrive,Sharepoint etc.)\n            R Programming\n            Python Programming\n            Julia Programming\n            MySQL\n            PostgreSQL\n            Blackboard\n            PeopleSoft\n            Facebook Marketing\n            Survey Data Collection Software (e.g. Questback, SurveyMonkey, GoogleForms etc.)\n            \n            \n            Publications\n            Nzimeni, S. Smit, AVA. 2018. ‘Is the quality of education impacting the global competitiveness of the South African business environment?’ 30th Annual Conference of the South African Institute of Management Scientists: Conference Proceedings, Stellenbosch University, Stellebosch, 16 - 19 September 2018.\n            Nzimeni, S. 2019. ‘Enrolment versus Attendance: A preliminary investigation into the cost of tutorials’. Siyaphumelela 2019: A Saide project. Johannesburg, 25 - 27 June 2019.\n            Muller, A. Nzimeni, S. Janse van Vuuren, Corlia. 2021. ‘Curriculum enhancement: reflections on the use of data, holistic student support and disciplinary skills development on a decade-long transformative journey’. 2021 University of the Free State Annual Teaching and Learning Conference.\n            \n          \n        \n      \n    \n\n    \n      \n        \n          \n            \n              \n            \n              Sivuyile Nzimeni\n            \n            \n              \n                \n                                    \n                    \n                      LinkedIn\n                    \n                  \n                                    \n                    \n                      Twitter\n                    \n                  \n                                    \n                    \n                      Email\n                    \n                  \n                                  \n              \n            \n            \n              I am a Data Analyst and Teaching and Learning Coordinator for the Faculty of Economic and Management Sciences at the University of the Free State. Previously worked as a part-time lecturer (Personal Selling), Research Assistant and Intern at the same institution. I have a demonstrated history of working in the higher education industry with expertise in managing a large cohort of employees, Coordinating software development, Data Analysis Financial management, Monitoring and Evaluation, Report Writing and Presentation. In addition, I am skilled in the R Programming Language, Python, KNIME and Julia Programming language among others.\n              \n              Qualifications\n              2016: Bachelor of Commerce (Honours) with specialisation in Entrepreneurial Management (University of the Free State)\n              2015: Bachelor of Arts majoring in Philosophy and Management (University of the Free State)\n              2011: National Senior Certificate (Hoerskool De Vos Malan)\n              \n              \n              Certifications\n              Completed Certifications\n              Year\n              Month\n              Course Description\n              Provider\n              2021\n              Sept\n              Learning Julia (Programming Language)\n              LinkedIn Learning\n              2021\n              Aug\n              Artificial Intelligence Tools and Concepts\n              LinkedIn\n              Learning\n              2021\n              Jun\n              Python for Everybody Specialisation\n              Coursera\n              2021\n              May\n              PHP Essential Training\n              LinkedIn Learning\n              2021\n              Mar\n              Python Data Structures\n              Coursera\n              2021\n              Feb\n              Programming for Everybody(Getting Started with Python)\n              Coursera\n              2020\n              Oct\n              Advertising on Facebook: Advanced\n              LinkedIn Learning\n              2020\n              Sept\n              Developing Data Products\n              Coursera\n              2020\n              Sept\n              PostreSQL Essential Training\n              LinkedIn Learning\n              2020\n              Sept\n              Practical Machine Learning\n              Coursera\n              2020\n              Aug\n              Advanced R Programming\n              Coursera\n              2020\n              Jun\n              Regression Models\n              Coursera\n              2020\n              Apr\n              Data Visualisation in R with ggplot2\n              LinkedIn Learning\n              2020\n              Jan\n              R Statistics Essential Training\n              LinkedIn\n              Learning\n              2019\n              Dec\n              The R Programming Environment\n              Coursera\n              2017\n              Dec\n              Supplemental Instruction Supervisor Training Workshop\n              Nelson Mandela University( The Supplemental Instruction National Office)\n              2017\n              Mar\n              Advanced Project Management (NQF Level 5)\n              Extribyte (Pty) Ltd\n              Public ServiceSector Education & Training Authority\n              \n              \n              Skills\n              Process Automation\n              Data Analysis\n              Management\n              Customer Service\n              Big Data\n              Project Management\n              Database Administration\n              Data Visualisation\n              Web Application\n              Artificial Intelligence\n              \n              \n              Technologies\n              Microsoft365 (Excel,Word,Powerpoint,OneDrive,Sharepoint etc.)\n              R Programming\n              Python Programming\n              Julia Programming\n              MySQL\n              PostgreSQL\n              Blackboard\n              PeopleSoft\n              Facebook Marketing\n              Survey Data Collection Software (e.g. Questback, SurveyMonkey, GoogleForms etc.)\n              \n              \n              Publications\n              Nzimeni, S. Smit, AVA. 2018. ‘Is the quality of education impacting the global competitiveness of the South African business environment?’ 30th Annual Conference of the South African Institute of Management Scientists: Conference Proceedings, Stellenbosch University, Stellebosch, 16 - 19 September 2018.\n              Nzimeni, S. 2019. ‘Enrolment versus Attendance: A preliminary investigation into the cost of tutorials’. Siyaphumelela 2019: A Saide project. Johannesburg, 25 - 27 June 2019.\n              Muller, A. Nzimeni, S. Janse van Vuuren, Corlia. 2021. ‘Curriculum enhancement: reflections on the use of data, holistic student support and disciplinary skills development on a decade-long transformative journey’. 2021 University of the Free State Annual Teaching and Learning Conference.\n              \n            \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2022-03-01T03:32:52+02:00"
    },
    {
      "path": "Scrape_DBE.html",
      "title": "Department of Basic Education: Schools Database",
      "description": "This post details the data scraping process for obtaining the schools database of from the Department of Basic Education in South Africa.\n",
      "author": [
        {
          "name": {},
          "url": "https://www.linkedin.com/in/sivuyile-nzimeni-98006ba0/"
        }
      ],
      "date": "December 30, 2021",
      "contents": "\n\nContents\nINTRODUCTION\nDATA SCRAPING\nDOWNLOADS, SO MANY DOWNLOADS.\nBRING YOUR BROOMS CAUSE IT’S A MESS!\n\nCONCLUSION\nREFERENCES\n\nINTRODUCTION\nA few years ago, I was a student pursuing Masters of Commerce degree. The topic had something to do with the relationship between Education, Labour and Business outcomes. My pursuit of the qualification is defunct. However, there are several artefacts worth writting up. In this post, we will discuss the South African Schools Database. The Department of Basic Education regularly publishes versions of the School Database including a number of notable variables such as the school’s location, contact information, number of learners and teachers etc. The data is published in a non-standardised matter. This makes it an interesting data wrangling task.\nDATA SCRAPING\nThe first hurdle is the volume of files publised on the DBE website. It is possible to download all 200 files by hand and save them to a directory of your choosing. However, such a process would be tedious and error prone(speaking from experience, ofcourse). The R-Programming language is a perfect companion for this task. To download the files, we can use two important packages, rvest and xml2.\n\n\nData_Sets <- read_html(\"https://www.education.gov.za/Programmes/EMIS/EMISDownloads.aspx\") %>% \n  html_elements(\"a\") %>% \n  html_attr(name = \"href\")\n\nData_Sets <- data.frame(dataset_links = Data_Sets) %>% \n  filter(str_detect(dataset_links,\"^[[:punct:]]Link\"),\n         str_detect(dataset_links,\"forcedownload\"))\n\nData_Sets <- Data_Sets %>% \n  mutate(dataset_links = paste0(\"https://www.education.gov.za/Programmes/EMIS/EMISDownloads\",\n                                dataset_links))\n\n\n\nFirstly, we specify the url on the read_html function. Thereafter, we use the html_elements function to point to the html tag of interest. In this case, we are interest in the “a” tag, specifically, the href (or link) attribute.\nWe store the result in a data.frame object and use a regular expression to filter for values that start with a punctuation followed by “Link” and values that contain the term “forcedownload”. Finally, we append the path to file (our base url). The resulting data.frame contains all 200 downloadable files.\nDOWNLOADS, SO MANY DOWNLOADS.\nWith the links in hand, we can tackle the next hurdle, downloading the files. Usually, we could use the commandline to download the files. For example, the single command below.\n\nwget -i some_text_file.txt\n\nHowever, the DBE datasets are saved as either xlsx or xls format with a prompt on click to download the file. There are probably ways around this issue. Luckily, xml2 has a convenient function to handle this issue. In the code below, we use a for-loop to download each file in our dataset and save them in a specified sub folder. To avoid a break in the for-loop when an error occurs, we add the try function. As an add-on, we print a statement after each download. Depending on your internet connection speed and the website’s response time, this script can take five minutes to download all the files.\n\n\nfor(i in seq_along(1:nrow(Data_Sets))){\n  try(download_xml(url = Data_Sets$dataset_links[[i]],\n               file = paste0(\"./Schools_Db/\",\"file_\",i,\".xlsx\")))\n  print(paste0(\"File \",i, \" downloaded \",\"proceeding to file \",i+1,\".\"))\n}\n\n\n\nBRING YOUR BROOMS CAUSE IT’S A MESS!\nAnother set of tasks to address is reading and cleaning the excel files. Using other software such as Excel or SPSS, these tasks would be cumbersome. Yet with R, Python or other programming languages, it possible handle more than one file at a time. Below, we use the readxl and purrr R packages to iteratively read and clean the files.\n\n\n# All_Excel_Reader --------------------------------------------------------\nall_excel <- function(path){\n  collect_sheets <- excel_sheets(path)\n  number_of_sheets <- 1:length(collect_sheets)\n  per_sheet <- list()\n  for(i in seq_along(number_of_sheets)){\n    per_sheet[[i]] <- read_xlsx(path = path,\n                                sheet = collect_sheets[i])\n  }\n  return(per_sheet)}\n# All_Masterlist ----------------------------------------------------------\nEDU_Dbs <- data.frame(master_list = list.files(path = \"./Schools_Db\",\n           full.names=TRUE,\n           pattern = \".xlsx\")) %>% \n  mutate(schools_db = map(master_list,all_excel))\n\nEDU_Dbs <- EDU_Dbs %>% \n  unnest(schools_db)\n\nEDU_Dbs$schools_db <- lapply(EDU_Dbs$schools_db,sapply,as.character)\n\nEDU_Dbs$schools_db <- lapply(EDU_Dbs$schools_db,as.data.frame)\n\nEDU_Dbs <- EDU_Dbs %>% \n  unnest(schools_db)\n\nEDU_Dbs <- EDU_Dbs %>% \n  clean_names()\n\n\n\nThe resulting data.frame contains hundreds of thousands of rows and nearly 60 columns. Interestingly, most of these variable are effectively differing naming conventions such as emisno = natemis = oldnatemis = newnatemis. The insistent naming conventions extend to other variables such as gps coordinates and centre details. The spectacularly unoptimised script is availablehere. The scraper script,cleaning script and downloaded xlsx files are all available on the this github report.\nCONCLUSION\nUsing R to clean data is a wise choice. This post highlighted an example of implementation on a relatively small dataset. The final dataset can be used to match school performance reports regularly published by the Department of Basic Education.\nREFERENCES\nWickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686\nSam Firke (2021). janitor: Simple Tools for Examining and Cleaning Dirty Data. R package version 2.1.0. https://CRAN.R-project.org/package=janitor\nHadley Wickham and Jennifer Bryan (2019). readxl: Read Excel Files. R package version 1.3.1. https://CRAN.R-project.org/package=readxl\nJeroen Ooms (2021). writexl: Export Data Frames to Excel ‘xlsx’ Format. R package version 1.4.0. https://CRAN.R-project.org/package=writexl\nHadley Wickham (2021). rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.2. https://CRAN.R-project.org/package=rvest\n\n\n\n",
      "last_modified": "2022-03-01T03:33:00+02:00"
    },
    {
      "path": "State_Capture_One.html",
      "title": "State of Capture Commission Report Part 1: Notes",
      "description": "This posts is the first installment of notes from reading the State of Capture Commission's Report Part 1. This post post focuses on the first few pages of the report. \n",
      "author": [
        {
          "name": {},
          "url": "https://www.linkedin.com/in/sivuyile-nzimeni-98006ba0/"
        }
      ],
      "date": "January 05, 2022",
      "contents": "\n\nContents\nBACKGROUND\nREADING THE REPORT\nNOTES\nSOUTH AFRICAN AIRWAYS\n\nCONCLUSION\n\nBACKGROUND\nOn 14 October 2016, the previous Public Protector, Adv Thuli Mandonsela, published the State of Capture Report. The famed public sector watchdog had a reputation for holding public officials accountable. The State of Capture report was no different.\nIt documented the pervasiveness of State Capture in the Jacob Zuma administrations. The report provided prima facie evidence of State Capture, including many public officials burning desire to visit the home of the notorious Gupta Brothers.\nThe report also lent credence to the assertion of several whistleblowers that Guptas were surreptitiously influencing key public-sector decisions. Most importantly, the report led to the Commission of Inquiry into State Capture.\nIn the years since, a flurry of litigation in the peripheral of State Capture, some directly related to the topic, ensued; Jacob Zuma resigned, the Adv Thuli Mandonsela completed her term as the Public Protector and moved on to Stellenbosch University, and the Commission commenced its work.\nOn 04 January 2022, the Commission published Volume 1 of 3 of its report. The 04 January 2022 report is a continuation of the initial report from the previous Public Protector. The Commission’s report provides comprehensive detail on the allegations of State Capture, investigating claims and issuing several findings against several parties.\nREADING THE REPORT\nThe 04 January 2022 report is the culmination of 430 hearings, 778 videos and 170 666 Affidavits and Statements, 3 171 summons issued to witness and 1 380 Requests For Information (State Capture.org.za 2022). Reading the report’s contents is important to understand the depth and breadth of the State Capture project. Beyond this, it is also important to read through the information to defend against disinformation and misinformation campaigns that may arise on social media especially considering the Bell Pottinger scandal.\nNOTES\nThis post attempts to summarise the report’s contents into layman’s language. It is also an attempt to find salient points throughout the report and ultimately to gain an in-depth understanding of the report.\nThe report opens with background information on the establishment of the Commission. It provides descriptive statistics on the work of the Commission and recounts the Commission’s mandate. The Commission heard evidence related to the following State-Owned Enterprises and Private Enterprises:\nSouth African Airways (SAA) and Subsidiaries\nBOSASA\nDenel\nEskom\nEstina Dairy Farm\nPRASA\nSABC\nSARS\nSSA\nTransnet.\nChapter 1 of the report focuses on SAA and its subsidiaries. Chapters 2, 3, and 4 focus on The New Age and its dealings with government departments and State-Owned Entities, South African Revenue Service and Public Procurement in South Africa, respectively. This post will focus on the first 140 pages of the report, with the remaining pages covered in subsequent posts.\nSOUTH AFRICAN AIRWAYS\nThe report is unambiguous about the entity’s state while Ms Dudu Myeni and Ms Kwinana served as board members. In short, the report asserts that SAA experienced a steady decline in quality and effectiveness. This decline is largely attributed to the tenures of Ms Myeni and Ms Kwinana. Throughout their terms, corruption and fraud were rampant at the institution. Those that opposed illegal conduct were victimised and eventually removed from their positions. The Commission also highlights failures by SAA auditors to detect fraud and corruption. The collapse also extends to SAA’s internal audit function. There was also a wholesale failure in governance within the SAA and its subsidiaries (see paragraphs 11 – 20 of the report).\nThe Commission is particularly scathing on the conduct of Ms Myeni, arguing that she created the hostile environment described above through “a mixture of negligence, incompetence and deliberate corrupt intent” to further the project of dismantling governance at the entity (see Paragraph 13 of the report).\nThe Commission also implicates several other stakeholders as enablers of Myeni’s conduct. These stakeholders include former President Jacob Zuma (see paragraph 14), former Ministers Gigaba and Brown (see paragraph 18). Finally, the Cabinet, the Executive of South Africa, is also implicated for yielding to the preferences of the President rather than the interests of State-Owned Enterprises. Ms Carolous (previous SAA Board Chair) detailed several instances where then Minister of Public Enterprises, Mr Malusi Gigaba, took steps to undermine the authority of the Board and affect the strategic outcomes of the entity. These actions include:\nExcluding the Board Chair from Communication\nPublicly denouncing the Board, calling them unpatriotic and incompetent\nDelaying submission of the Request for a Guarantee to Treasury\nMisleading the Board Chair about the timeline of the proposal to Treasury Pressuring SAA to close the commercially viable Mumbai route.\nAppointing Ms Dudu Myeni as Board Chair despite her poor attendance record at SAA Board Meetings. (see Paragraphs 39 – 68)\nThe report also reveals that Mr Siyabonga Mahlangu, then adviser to the Minister of Public Enterprises, had an unquenchable thirst to visit the Gupta household. Mr Mahlangu’s affections for the Saxonwold compound also included inviting others to the address.\nOne such invitee was then Acting Chief Executive Officer, Mr Kona. Mr Kona alleges that Mr Tony Gupta offered him a large amount of money, which he refused. At the same meeting, Mr Kona informed Mr Tony Gupta about allocating a tender to Lufthansa Consultancy. In addition, Mr Kona alleged that Mr Tony Gupta was livid about the tender allocation.\nAfter the visit, Mr Kona received a call from the Director-General of Public Enterprises, Mr Tshediso Matona. The Director-General expressed his displeasure on the tender allocation to Lufthansa Consultancy. The grievances of the Director-General were to the extent that the Department of Public Enterprises deemed it necessary to investigate the tender. The investigation found no irregularities associated with the tender. Despite the findings of the investigation, the Department of Public Enterprises refused the continuance of the tender.\nCONCLUSION\nThis is the tip of the iceberg. There several specific allegation against Ms Dudu Myeni and Ms Kwinana. These will be discussed in subsequent summaries (trying to keep the word count below 1000). For now, it clear that State Capture was (or is) present at South African Airways. The following post will detail specific incidents of how State Capture took hold of the entity with specific reference to the conduct of Ms Dudu Myeni.\n\n\n\n",
      "last_modified": "2022-03-01T03:33:01+02:00"
    }
  ],
  "collections": []
}
