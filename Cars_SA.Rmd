---
title: "South African Car Prices"
description: |
  This post details the data scraping process for obtaining the schools database of from the Department of Basic Education in South Africa.
date: February 26, 2022
author:
  - first_name: "Sivuyile"
    last_name: "Nzimeni"
    url: https://www.linkedin.com/in/sivuyile-nzimeni-98006ba0/
output:
  distill::distill_article:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      error = FALSE)
lapply(as.list(c("tidyverse","janitor","readxl","writexl",
                "ggthemes","tidymodels","gt",
                "gtExtras","textrecipes",
                "tidytext","modeldata",
                "glmnet","h2o","xgboost")),
       require,character.only=TRUE)
theme_set(theme_clean())
doParallel::registerDoParallel(core=6)
```

# INTRODUCTION

South Africa has a plethora of online vehicle marketplaces. Often, their pool of vehicles for sale are usually > 50 000 on a daily basis. The vehicle listings often a vast amount of car related data. Naturally, web-scrapping the data provides an opportunity to fit a Machine-Learning model and endless exploration for petrol-heads (such as myself). Nearly all the online vehicle marketplaces have restrictive Terms and Conditions deterring the use of their data for commercial purposes and bombarding of their servers through web scrapping among other restrictions. Unfortunately, this means web scrapping script cannot be shared in this post as it may reveal where the data were obtained. In addition, the website of the online vehicle marketplace will not be revealed. 

# Web Scrapping

To respect the Terms of Conditions, the scrapping was throttled through the use of base R's `rpois` function along with the `Sys.sleep()` function. Combining the two functions results in non-normal distribution of system sleep. System sleep essentially pauses R for the specified period. This step in the scrapping process is important to avoid sending to many requests to a website, perhaps, hoarding server capacity for other users. In this instance,scrapping all the webpages and pulling all listed vehicles on the day(2021-09-02), took 10 hours. 

```{r Throttling,fig.cap="Example of using sys.sleep with rpois",eval=FALSE}
pause <- function(){
  Sys.sleep(rpois(1,60))
  print(paste0("Proceeding to the next page..."))
  }
```

# IMPORTING THE DATA

Below, we import the data and use `dplyr::glimpse` function to see the number of columns and values. The dataset contains a few important variables including the car_name and vehicle manufacturer. It is worth noting that the car_name variable appears to a free text field where the person listing the vehicle can modify the car name to include marketing terms such as: "excellent condition","reduce price" etc.

```{r Import_Data,fig.cap="Importing the dataset and dplyr::glimpse"}
Car_Data <- read_csv(file ="./2021-09-03_MANY_VEHICLES_Clean_Db.csv") %>% 
  clean_names()

glimpse(Car_Data)
```


Similarly, the free text field, also means that vehicle naming conventions deviate from the vehicle manufacturer specifications. Other text variables also contain these anomalous changes in text. The code below demonstrates an example of idiosyncrasies.

```{r Peculiar,fig.cap="Perculiar terms in text variables"}
Car_Data %>% 
  filter(str_detect(car_name,"condition"))
```

# DATA PREPROCESSING

We would like to fit a model to better understand the determinants of vehicle prices. Here, pre-processing is important is especially important. `Tidymodels` and the `textrecipes` offer a range of functions to handle the whole modelling workflow. Below, are a number of pre-processing steps, firstly we use the `unnest_tokens` function from the `tidytext package` to process the the `car_name` variable. Thereafter, we handle the numerical values by applying a logarithm to outcome variable and standardising mileage. Finally, we use `step_tokenize`, `step_tokenfilter` and `step_tf` to tokenise, filter the those tokens and ultimately convert the tokens to a term frequency variables. 

```{r Pre-processing,fig.cap="Tidymodels preprocessing"}
Useful_Car_Names <- Car_Data %>% 
  unnest_tokens(car_name,
                output="car_name") %>% 
  group_by(vehicle_manufacturer,car_name) %>% 
  summarise(n(),.groups = "drop") %>% 
  arrange(desc(`n()`)) %>% 
  anti_join(stop_words %>% 
              rename(car_name=word)) %>% 
  filter(!str_detect(car_name,"\\d{1,}"))

Car_Data <- Car_Data %>% 
  unnest_tokens(car_name,output = "car_name") %>% 
  semi_join(Useful_Car_Names %>% 
              select(car_name,vehicle_manufacturer)) %>%
  group_by(across(-car_name)) %>% 
  summarise(car_name = as.character(list(c(car_name))),
            .groups = "drop") %>% 
  mutate(car_name = str_replace(car_name,
                                'c[[:punct:]]{2,}',""),
         car_name = str_replace_all(car_name,
                                    '\\"',""),
         car_name = str_replace_all(car_name,
                                    '[[:punct:]]{1,}$',""),
         age = 2021-year)
Car_Data <- Car_Data %>% 
  mutate(across(c(car_name,dealership,city_town),
                .fns = as_factor))

New_Car_Data <- recipe(price ~ .,data = Car_Data) %>% 
  step_log(all_outcomes()) %>%
  step_normalize(mileage) %>% 
  step_tokenize(c(dealership,city_town,car_name)) %>%
  step_tokenfilter(c(dealership,city_town,car_name)) %>%
  step_tf(c(dealership,city_town,car_name)) %>% 
  step_dummy(c(vehicle_manufacturer,province,
               fuel_type,transmission)) %>% 
  step_nzv(all_predictors()) %>% 
  prep() %>% 
  bake(Car_Data)
```
 
# LASSO IT ONCE, LASSO IT UNTIL YOU CAN ALSO NO MORE

The resulting dataset contains 59417 rows across 308 variables. Given the dimension above, it prudent to do some additional feature select. LASSO regression helps with variable selection.

```{r LASSO_Reg}
New_Car_Data <- New_Car_Data %>%
  select(-year)

Car_split <- initial_split(New_Car_Data)
Car_Training <- training(Car_split)
Car_Test <- testing(Car_split)

X <- model.matrix(price~.,Car_Training)[,-1]
Y <- Car_Training$price
lasso_model <- glmnet(x = X,y=Y,
       alpha=1)
```

```{r}
variable_extractor <- function(a_list){
  min_lambda <- data.frame(best_lambda =a_list[["lambda"]]==min(a_list[["lambda"]]))
  min_lambda <- cbind.data.frame(data.frame(index = rownames(min_lambda)),
                                 min_lambda)
  min_lambda <- min_lambda %>% 
    filter(best_lambda == TRUE)
  min_lambda <- as.double(unique(min_lambda$index))
  lasso_variables <-
    as.matrix(coef(a_list))|>data.frame()
  
  lasso_variables <- cbind.data.frame(variables = rownames(lasso_variables),
                                      lasso_variables)
  
  lasso_variables <- tibble(lasso_variables)
  lasso_variables <- lasso_variables[,c(1,min_lambda+1)]
  names(lasso_variables) <- c("variable","importance")
  lasso_variables <- lasso_variables %>% 
    filter(variable != "(Intercept)",
           importance != 0.00000000) %>% 
    arrange(desc(importance))

  return(lasso_variables)
}
```

```{r}
Variables <- variable_extractor(lasso_model)
Car_Training <-Car_Training[,c("price",Variables$variable)]
Car_Test <-Car_Test[c("price",Variables$variable)]
```

```{r}
lm(price ~.,Car_Training) %>% 
  summary()
```

